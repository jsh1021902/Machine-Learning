{"cells":[{"cell_type":"markdown","metadata":{"id":"52zmK05Peo9Z"},"source":["# 문제 "]},{"cell_type":"markdown","metadata":{"id":"yUbxvkFCeq-a"},"source":["categorical feature로 구성된 입력 값을 사용하는 Naive Bayes Classifier를 구현  \n","models.py에서 Fill-in으로 처리된 부분들 (fit 과 predict) 부분을 채워주면 됩니다.  \n","Categorical feature의 경우 어떻게 Naive Bayes Classifier를 만들 수 있는 지 수식을 참고\n","\n","(참고로, Naive Bayes Classifier는 확률은 count기반으로 어림하기 때문에, 학습 시 랜덤한 특성이 없음. 따라서, source code와 동일한 결과를 내야 함.)\n","\n","모든 결과는 smoothing factor m=1로 설정"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680689305300,"user":{"displayName":"럭키준","userId":"02668651701637114574"},"user_tz":-540},"id":"uuwK1n4TeYdC"},"outputs":[],"source":["import numpy as np\n","\n","class NaiveBayesClassifier:\n","    def __init__(self):\n","        self.prior = None\n","        self.conditional_prob = None\n","        self.m = 1 #smoothing factor\n","    \n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","        self.classes = np.unique(y)\n","        n_classes = len(self.classes)\n","\n","        # Compute prior probabilities of each class: 'self.prior'\n","        self.prior = np.zeros(n_classes)\n","  \n","        for idx, c in enumerate(self.classes):\n","            self.prior[idx] = np.sum(y == c) / n_samples\n","\n","        # Compute conditional probabilities of each feature given each class:  'self.conditional_prob\n","        self.conditional_prob = np.zeros((n_classes, n_features), dtype=np.ndarray)\n","\n","        # use file type dict() for each element in self.conditional_prob  (this will be an easier way to find the frequency for each category of a given feature)\n","        for c_idx, c in enumerate(self.classes):\n","            samples_in_class = X[y == c]\n","            for f_idx in range(n_features):\n","                self.conditional_prob[c_idx, f_idx] = dict()\n","                unique_vals, counts = np.unique(samples_in_class[:, f_idx], return_counts=True)\n","                total_counts = np.sum(counts) + self.m * len(unique_vals)\n","                for u_val, count in zip(unique_vals, counts):\n","                    self.conditional_prob[c_idx, f_idx][u_val] = (count + self.m) / total_counts\n","        return self\n","        \n","    def predict(self, X):\n","        y_pred = np.zeros(len(X), dtype=np.int8)\n"," \n","        for i, sample in enumerate(X):\n","            probabilities = np.zeros(len(self.classes))\n","            for c_idx, c in enumerate(self.classes):\n","                prob = np.log(self.prior[c_idx])\n","                for f_idx, val in enumerate(sample):\n","                    if val in self.conditional_prob[c_idx, f_idx]:\n","                        prob += np.log(self.conditional_prob[c_idx, f_idx][val])\n","                probabilities[c_idx] = prob\n","            y_pred[i] = self.classes[np.argmax(probabilities)]\n","            \n","        return y_pred"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1680688462288,"user":{"displayName":"럭키준","userId":"02668651701637114574"},"user_tz":-540},"id":"751zBWAeyyCF"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","class NaiveBayesClassifier:\n","    def __init__(self):\n","        self.priors = {}\n","        self.likelihoods = {}\n","        self.m = 1\n","\n","    def fit(self, X, y):\n","        # Calculate priors\n","        classes = set(y)\n","        n = len(y)\n","        for c in classes:\n","            self.priors[c] = sum(1 for i in y if i == c) / n\n","\n","        # Calculate likelihoods\n","        for i in range(len(X[0])):\n","            for c in classes:\n","                self.likelihoods[(i, c)] = {}\n","                feature_values = set([X[j][i] for j in range(n)])\n","                for f in feature_values:\n","                    self.likelihoods[(i, c)][f] = (\n","                        (sum(1 for j in range(n) if X[j][i] == f and y[j] == c) + self.m) /\n","                        (sum(1 for j in range(n) if y[j] == c) + self.m * len(classes))\n","                    )\n","\n","    def predict(self, X):\n","        y_pred = []\n","        for example in X:\n","            posteriors = {}\n","            for c in self.priors:\n","                likelihood = 1\n","                for i in range(len(example)):\n","                    if (i, c) in self.likelihoods:\n","                        likelihood *= self.likelihoods[(i, c)][example[i]]\n","                posteriors[c] = self.priors[c] * likelihood\n","            y_pred.append(max(posteriors, key=posteriors.get))\n","        return y_pred\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1680689318584,"user":{"displayName":"럭키준","userId":"02668651701637114574"},"user_tz":-540},"id":"NFbSp77EelsC","outputId":"a715ea46-26ae-4d06-d43a-91035a0ab239"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0]]\n","0.7619327924653853\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import sys\n","\n","sys.path.append('/content/drive/My Drive/응용통계학과/기계학습/Project1_Final')                                      # 구글 드라이브 경로 추가\n","from models import NaiveBayesClassifier\n","\n","# import data\n","tr_df = pd.read_csv('/content/drive/My Drive/응용통계학과/기계학습/Project1_Final/train_data.csv')\n","te_df = pd.read_csv('/content/drive/My Drive/응용통계학과/기계학습/Project1_Final/train_data.csv')\n","\n","# split feature/labels\n","feat_list = ['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capitalgain', 'capitalloss', 'hoursperweek']\n","label     = 'class'\n","\n","tr_X      = np.asarray(tr_df[feat_list])\n","tr_y      = np.asarray(tr_df[label])\n","\n","te_X      = np.asarray(te_df[feat_list])\n","te_y      = np.asarray(te_df[label])\n","\n","\n","model = NaiveBayesClassifier()\n","\n","model.fit(tr_X,tr_y)\n","\n","print(model.conditional_prob)\n","print(np.mean(model.predict(te_X) == te_y))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1493,"status":"ok","timestamp":1680689123860,"user":{"displayName":"럭키준","userId":"02668651701637114574"},"user_tz":-540},"id":"o4qI3IRNialW","outputId":"55431334-4501-4a57-b971-83351f19cf60"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7648413510747185\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","sys.path.append('/content/drive/My Drive/응용통계학과/기계학습/Project1_Final')                                      # 구글 드라이브 경로 추가\n","\n","from models import NaiveBayesClassifier\n","\n","# import data\n","tr_df = pd.read_csv('/content/drive/My Drive/응용통계학과/기계학습/Project1_Final/train_data.csv')\n","# te_df = pd.read_csv('./test_data.csv')\n","\n","from sklearn.model_selection import train_test_split\n","tr_df, te_df = train_test_split(tr_df, test_size=0.1, random_state=48)\n","\n","# split feature/labels\n","feat_list = ['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capitalgain', 'capitalloss', 'hoursperweek']\n","label     = 'class'\n","\n","tr_X      = np.asarray(tr_df[feat_list])\n","tr_y      = np.asarray(tr_df[label])\n","\n","te_X      = np.asarray(te_df[feat_list])\n","te_y      = np.asarray(te_df[label])\n","\n","\n","\n","model = NaiveBayesClassifier()\n","\n","model.fit(tr_X,tr_y)\n","print(np.mean(model.predict(te_X) == te_y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yvc62JHt3-bA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM9wXvSh/hr7UrY/W1FqqJ+","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
